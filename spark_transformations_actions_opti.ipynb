{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext('local', 'PySpark - SQL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://archive.ics.uci.edu/ml/machine-learning-databases/kddcup99-mld/kddcup.data.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = sc.textFile(\"kddcup.data.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row, SQLContext\n",
    "sql_context = SQLContext(sc)\n",
    "csv = raw_data.map(lambda l: l.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = csv.map(lambda p: Row(duration=int(p[0]), protocol=p[1], service=p[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary Tables\n",
    "df = sql_context.createDataFrame(rows)\n",
    "df.registerTempTable(\"rdd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|duration|\n",
      "+--------+\n",
      "|   10217|\n",
      "|   11610|\n",
      "|   13724|\n",
      "|   10934|\n",
      "|   12026|\n",
      "|    5506|\n",
      "|   12381|\n",
      "|    9733|\n",
      "|   17932|\n",
      "|   40504|\n",
      "|   11565|\n",
      "|   12454|\n",
      "|    9473|\n",
      "|   12865|\n",
      "|   11288|\n",
      "|   10501|\n",
      "|   14479|\n",
      "|   10774|\n",
      "|   10007|\n",
      "|   12828|\n",
      "+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_context.sql(\"\"\"SELECT duration FROM rdd WHERE protocol = 'tcp' AND duration > 2000\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|duration|\n",
      "+--------+\n",
      "|   10217|\n",
      "|   11610|\n",
      "|   13724|\n",
      "|   10934|\n",
      "|   12026|\n",
      "|    5506|\n",
      "|   12381|\n",
      "|    9733|\n",
      "|   17932|\n",
      "|   40504|\n",
      "|   11565|\n",
      "|   12454|\n",
      "|    9473|\n",
      "|   12865|\n",
      "|   11288|\n",
      "|   10501|\n",
      "|   14479|\n",
      "|   10774|\n",
      "|   10007|\n",
      "|   12828|\n",
      "+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"duration\").filter(df.duration>2000).filter(df.protocol==\"tcp\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, duration: string, protocol: string, service: string]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing - Transformations (Scala)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```scala\n",
    "//TESTING:  Setup in Scala\n",
    "class DeferComputations extends FunSuite {\n",
    "    val spark: SparkContext = SparkSession\n",
    "                                .builder()\n",
    "                                .master(\"local[2]\")\n",
    "                                .getOrCreate().sparkContext\n",
    "\n",
    "\n",
    "//ACTUAL TEST\n",
    "test(\"should defer computations\") {\n",
    " //given\n",
    "    val input = spark.makeRDD(\n",
    "        List(InputRecord(userId = \"A\"),\n",
    "            InputRecord(userId = \"B\"))) \n",
    "    \n",
    "case class InputRecord(userId: String)\n",
    "    \n",
    "    \n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```scala\n",
    "//when apply transformation\n",
    "val rdd = input\n",
    "    .filter(_.userId.contains(\"A\"))\n",
    "    .keyBy(_.userId)\n",
    ".map(_._2.userId.toLowerCase)\n",
    "//.... built processing graph lazy\n",
    "\n",
    "private def shouldExecutePartOfCode(): Boolean = {\n",
    "    //some domain logic that decides if we still need to calculate\n",
    "    true\n",
    "}\n",
    "\n",
    "if (shouldExecutePartOfCode()) {\n",
    "     //rdd.saveAsTextFile(\"\") ||\n",
    "     rdd.collect().toList\n",
    "  } else {\n",
    "    //condition changed - don't need to evaluate DAG\n",
    " }\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```scala\n",
    "\n",
    "case class UserTransaction(userId: String, amount: Int)\n",
    "\n",
    "test(\"should trigger computations using actions\") {\n",
    " //given\n",
    " val input = spark.makeRDD(\n",
    "     List(\n",
    "         UserTransaction(userId = \"A\", amount = 1001),\n",
    "         UserTransaction(userId = \"A\", amount = 100),\n",
    "         UserTransaction(userId = \"A\", amount = 102),\n",
    "         UserTransaction(userId = \"A\", amount = 1),\n",
    "         UserTransaction(userId = \"B\", amount = 13)))\n",
    "    \n",
    "\n",
    "// To get transactions for a specific userId \n",
    "//when apply transformation\n",
    "val rdd = input\n",
    "    .groupBy(_.userId)\n",
    "    .map(x => (x._1,x._2.toList))\n",
    "    .collect()\n",
    "    .toList\n",
    "\n",
    "//then\n",
    "rdd should contain theSameElementsAs List(\n",
    "    (\"B\", List(UserTransaction(\"B\", 13))),\n",
    "    (\"A\", List(\n",
    "        UserTransaction(\"A\", 1001),\n",
    "        UserTransaction(\"A\", 100),\n",
    "        UserTransaction(\"A\", 102),\n",
    "        UserTransaction(\"A\", 1))\n",
    "    )\n",
    "  )\n",
    " }\n",
    "}\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using reduce and reduceByKey instead of groupBy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```scala\n",
    "test(\"should use reduce API\") {\n",
    "    //given\n",
    "    val input = spark.makeRDD(List(\n",
    "    UserTransaction(\"A\", 10),\n",
    "    UserTransaction(\"B\", 1),\n",
    "    UserTransaction(\"A\", 101)\n",
    "    ))\n",
    "    \n",
    "     //when\n",
    "val result = input\n",
    "    .map(_.amount)\n",
    "    .reduce((a, b) => if (a > b) a else b)\n",
    "\n",
    "//then\n",
    "assert(result == 101)\n",
    "}\n",
    "\n",
    "\n",
    "test(\"should use reduceByKey API\") {\n",
    "    //given\n",
    "    val input = spark.makeRDD(\n",
    "    List(\n",
    "        UserTransaction(\"A\", 10),\n",
    "        UserTransaction(\"B\", 1),\n",
    "        UserTransaction(\"A\", 101)\n",
    "    )\n",
    ")\n",
    "    \n",
    "   \n",
    "    //when\n",
    "    val result = input\n",
    "      .keyBy(_.userId)\n",
    "      .reduceByKey((firstTransaction, secondTransaction) =>\n",
    "        TransactionChecker.higherTransactionAmount(firstTransaction, secondTransaction))\n",
    "      .collect()\n",
    "      .toList\n",
    "    \n",
    "    \n",
    "        //then\n",
    "    result should contain theSameElementsAs\n",
    "      List((\"B\", UserTransaction(\"B\", 1)), (\"A\", UserTransaction(\"A\", 101)))\n",
    "  }\n",
    "\n",
    "}\n",
    "    \n",
    "  \n",
    "    \n",
    "object TransactionChecker {\n",
    "    def higherTransactionAmount(firstTransaction: UserTransaction,\n",
    "                                secondTransaction: UserTransaction): UserTransaction = {\n",
    "        if (firstTransaction.amount > secondTransaction.amount) firstTransaction else  secondTransaction\n",
    "    }\n",
    "}\n",
    "    \n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions - Computations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```scala \n",
    "\n",
    "test(\"should trigger computations using actions\") {\n",
    "     //given\n",
    "     val input = spark.makeRDD(\n",
    "     List(\n",
    "         UserTransaction(userId = \"A\", amount = 1001),\n",
    "         UserTransaction(userId = \"A\", amount = 100),\n",
    "         UserTransaction(userId = \"A\", amount = 102),\n",
    "         UserTransaction(userId = \"A\", amount = 1),\n",
    "         UserTransaction(userId = \"B\", amount = 13)))\n",
    "\n",
    "//when apply transformation\n",
    " val rdd = input\n",
    "     .filter(_.userId.contains(\"A\"))\n",
    "     .keyBy(_.userId)\n",
    "     .map(_._2.amount)\n",
    "    \n",
    "//then\n",
    " println(rdd.collect().toList)\n",
    " println(rdd.count()) //and all count*\n",
    "    \n",
    "    \n",
    "// Simple check\n",
    " println(rdd.first())\n",
    " \n",
    "    \n",
    " rdd.foreach(println(_))\n",
    " rdd.foreachPartition(t => t.foreach(println(_)))\n",
    " println(rdd.max())\n",
    " println(rdd.min())\n",
    " \n",
    " //takeOrdered() - needs to execute DAT and sort everything [TIME CONSUMING!!!]\n",
    " println(rdd.takeOrdered(1).toList)\n",
    " println(rdd.takeSample(false, 2).toList)\n",
    " }\n",
    "}\n",
    "    \n",
    "    \n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reuse rdd to minimize time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```scala\n",
    "\n",
    "//then every call to action means that we are going up to the RDD chain\n",
    "//if we are loading data from external file-system (I.E.: HDFS), every action means\n",
    "//that we need to load it from FS.\n",
    "    val start = System.currentTimeMillis()\n",
    "    println(rdd.collect().toList)\n",
    "    println(rdd.count())\n",
    "    println(rdd.first())\n",
    "    rdd.foreach(println(_))\n",
    "    rdd.foreachPartition(t => t.foreach(println(_)))\n",
    "    println(rdd.max())\n",
    "    println(rdd.min())\n",
    "    println(rdd.takeOrdered(1).toList)\n",
    "    println(rdd.takeSample(false, 2).toList)\n",
    "    val result = System.currentTimeMillis() - start\n",
    "\n",
    "    println(s\"time taken (no-cache): $result\")\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "// Using Caching\n",
    "//when apply transformation\n",
    "val rdd = input\n",
    "    .filter(_.userId.contains(\"A\"))\n",
    "    .keyBy(_.userId)\n",
    "    .map(_._2.amount)\n",
    "    .cache()\n",
    "    \n",
    "//Rerun test\n",
    "//then every call to action means that we are going up to the RDD chain\n",
    "//if we are loading data from external file-system (I.E.: HDFS), every action means\n",
    "//that we need to load it from FS.\n",
    "    val start = System.currentTimeMillis()\n",
    "    println(rdd.collect().toList)\n",
    "    println(rdd.count())\n",
    "    println(rdd.first())\n",
    "    rdd.foreach(println(_))\n",
    "    rdd.foreachPartition(t => t.foreach(println(_)))\n",
    "    println(rdd.max())\n",
    "    println(rdd.min())\n",
    "    println(rdd.takeOrdered(1).toList)\n",
    "    println(rdd.takeSample(false, 2).toList)\n",
    "    val result = System.currentTimeMillis() - start\n",
    "\n",
    "    println(s\"time taken(cache): $result\")\n",
    "\n",
    "\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
